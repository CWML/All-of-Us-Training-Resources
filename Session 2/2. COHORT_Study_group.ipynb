{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook File Overview\n",
    "This notebook features two different SQL queries based on the Study cohort we created (has diabetes) along with the parameters we set for the variables we selected from the concept sets and dataset builder step.\n",
    "\n",
    "**The first SQL query extracts our demographic variables we chose**\n",
    "\n",
    "**The second SQL query extracts the survey question variables we chose**\n",
    "\n",
    "This *Study* Cohort should have the following characteristics:\n",
    "* Age 25-50\n",
    "* Race: White, Black, & Asian\n",
    "* Ethnicity: Hispanic Yes or No\n",
    "* Gender: Male or Female\n",
    "* Condition: YES Diabetes\n",
    "* PTSD survey questions: Currently seeing a doctor? & Currently prescribed meds?\n",
    "\n",
    "Each query is autogenerated from the details we provided during the cohort builder and dataset builder phase which creates the tables based on the Concept Sets we selected:\n",
    "1.\tDemographics\n",
    "2.\tSurvey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Code Chunk to Extract Demographic Variables\n",
    "\n",
    "This part is an SQL query which extracts the demographic variables we selected in the dataset builder filtered and subsetted by the parameters we chose using the Cohort Builder:\n",
    "\n",
    "* Age-group (25-50)\n",
    "* Ethnicity (Hispanic Yes/No)\n",
    "* Race (White, Black, or Asian)\n",
    "* Gender (Male or Female)\n",
    "* Inclusion criteria (we INCLUDE those with diabetes)\n",
    "\n",
    "**The following code chunks perform three key steps:**\n",
    "\n",
    "1. **Creates the SQL query as a text string** (does not execute the query yet)\n",
    "2. **Creates the file path** and displays it\n",
    "3. **Executes the BigQuery export** which runs the query and saves results as a CSV file to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"diabetes_study_ptsd\" for domain \"person\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "# The paste() function probably has smart quotes. Try this version:\n",
    "dataset_89304976_person_sql <- paste(\"\n",
    "    SELECT\n",
    "        person.person_id,\n",
    "        p_gender_concept.concept_name as gender,\n",
    "        person.birth_datetime as date_of_birth,\n",
    "        p_race_concept.concept_name as race,\n",
    "        p_ethnicity_concept.concept_name as ethnicity \n",
    "    FROM\n",
    "        `person` person \n",
    "    LEFT JOIN\n",
    "        `concept` p_gender_concept \n",
    "            ON person.gender_concept_id = p_gender_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_race_concept \n",
    "            ON person.race_concept_id = p_race_concept.concept_id \n",
    "    LEFT JOIN\n",
    "        `concept` p_ethnicity_concept \n",
    "            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id  \n",
    "    WHERE\n",
    "        person.PERSON_ID IN (SELECT\n",
    "            distinct person_id  \n",
    "        FROM\n",
    "            `cb_search_person` cb_search_person  \n",
    "        WHERE\n",
    "            cb_search_person.person_id IN (SELECT\n",
    "                person_id \n",
    "            FROM\n",
    "                `cb_search_person` p \n",
    "            WHERE\n",
    "                DATE_DIFF(CURRENT_DATE, dob, YEAR) - IF(EXTRACT(MONTH FROM dob)*100 + EXTRACT(DAY FROM dob) > EXTRACT(MONTH FROM CURRENT_DATE)*100 + EXTRACT(DAY FROM CURRENT_DATE), 1, 0) BETWEEN 25 AND 50 \n",
    "                AND NOT EXISTS (      SELECT\n",
    "                    'x'      \n",
    "                FROM\n",
    "                    `death` d      \n",
    "                WHERE\n",
    "                    d.person_id = p.person_id ) ) \n",
    "            AND cb_search_person.person_id IN (SELECT\n",
    "                person_id \n",
    "            FROM\n",
    "                `person` p \n",
    "            WHERE\n",
    "                ethnicity_concept_id IN (38003564, 38003563) ) \n",
    "            AND cb_search_person.person_id IN (SELECT\n",
    "                person_id \n",
    "            FROM\n",
    "                `person` p \n",
    "            WHERE\n",
    "                gender_concept_id IN (45878463, 45880669) ) \n",
    "            AND cb_search_person.person_id IN (SELECT\n",
    "                person_id \n",
    "            FROM\n",
    "                `person` p \n",
    "            WHERE\n",
    "                race_concept_id IN (8527, 8516, 8515) ) \n",
    "            AND cb_search_person.person_id IN (SELECT\n",
    "                criteria.person_id \n",
    "            FROM\n",
    "                (SELECT\n",
    "                    DISTINCT person_id, entry_date, concept_id \n",
    "                FROM\n",
    "                    `cb_search_all_events` \n",
    "                WHERE\n",
    "                    (concept_id IN(SELECT\n",
    "                        DISTINCT c.concept_id \n",
    "                    FROM\n",
    "                        `cb_criteria` c \n",
    "                    JOIN\n",
    "                        (SELECT\n",
    "                            CAST(cr.id as string) AS id       \n",
    "                        FROM\n",
    "                            `cb_criteria` cr       \n",
    "                        WHERE\n",
    "                            concept_id IN (201820)       \n",
    "                            AND full_text LIKE '%_rank1]%'      ) a \n",
    "                            ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                            OR c.path LIKE CONCAT('%.', a.id) \n",
    "                            OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                            OR c.path = a.id) \n",
    "                    WHERE\n",
    "                        is_standard = 1 \n",
    "                        AND is_selectable = 1) \n",
    "                    AND is_standard = 1 )) criteria ) )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "person_89304976_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"person_89304976\",\n",
    "  \"person_89304976_*.csv\")\n",
    "message(str_glue('The data will be written to {person_89304976_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_89304976_person_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  person_89304976_path,\n",
    "  destination_format = \"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {person_89304976_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_89304976_person_df <- read_bq_export_from_workspace_bucket(person_89304976_path)\n",
    "\n",
    "dim(dataset_89304976_person_df)\n",
    "\n",
    "head(dataset_89304976_person_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Code Chunk to Extract Survey Variables\n",
    "\n",
    "This SQL query extracts the survey variables we selected in the dataset builder concept sets:\n",
    "\n",
    "* Survey question 1: Currently seeing a doctor?\n",
    "* Survey question 2: Currently prescribed meds?\n",
    "\n",
    "The results are filtered and subsetted by the parameters we chose using the Cohort Builder:\n",
    "\n",
    "* Age group (25-50)\n",
    "* Ethnicity (Hispanic Yes/No)\n",
    "* Race (White, Black, or Asian)\n",
    "* Gender (Male or Female)\n",
    "* Inclusion criteria (**INCLUDES** those with diabetes)\n",
    "\n",
    "**The following code chunks perform three key steps:**\n",
    "\n",
    "1. **Creates the SQL query as a text string** (does not execute the query yet)\n",
    "2. **Creates the file path** and displays it\n",
    "3. **Executes the BigQuery export** which runs the query and saves results as a CSV file to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(bigrquery)\n",
    "\n",
    "# This query represents dataset \"diabetes_study_ptsd\" for domain \"survey\" and was generated for All of Us Controlled Tier Dataset v8\n",
    "dataset_89304976_survey_sql <- paste(\"\n",
    "    SELECT\n",
    "        answer.person_id,\n",
    "        answer.question,\n",
    "        answer.answer  \n",
    "    FROM\n",
    "        `ds_survey` answer   \n",
    "    WHERE\n",
    "        (\n",
    "            question_concept_id IN (43528846, 43530361)\n",
    "        )  \n",
    "        AND (\n",
    "            answer.PERSON_ID IN (SELECT\n",
    "                distinct person_id  \n",
    "            FROM\n",
    "                `cb_search_person` cb_search_person  \n",
    "            WHERE\n",
    "                cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `cb_search_person` p \n",
    "                WHERE\n",
    "                    DATE_DIFF(CURRENT_DATE, dob, YEAR) - IF(EXTRACT(MONTH FROM dob)*100 + EXTRACT(DAY FROM dob) > EXTRACT(MONTH FROM CURRENT_DATE)*100 + EXTRACT(DAY FROM CURRENT_DATE), 1, 0) BETWEEN 25 AND 50 \n",
    "                    AND NOT EXISTS (      SELECT\n",
    "                        'x'      \n",
    "                    FROM\n",
    "                        `death` d      \n",
    "                    WHERE\n",
    "                        d.person_id = p.person_id ) ) \n",
    "                AND cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `person` p \n",
    "                WHERE\n",
    "                    ethnicity_concept_id IN (38003564, 38003563) ) \n",
    "                AND cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `person` p \n",
    "                WHERE\n",
    "                    gender_concept_id IN (45878463, 45880669) ) \n",
    "                AND cb_search_person.person_id IN (SELECT\n",
    "                    person_id \n",
    "                FROM\n",
    "                    `person` p \n",
    "                WHERE\n",
    "                    race_concept_id IN (8527, 8516, 8515) ) \n",
    "                AND cb_search_person.person_id IN (SELECT\n",
    "                    criteria.person_id \n",
    "                FROM\n",
    "                    (SELECT\n",
    "                        DISTINCT person_id, entry_date, concept_id \n",
    "                    FROM\n",
    "                        `cb_search_all_events` \n",
    "                    WHERE\n",
    "                        (concept_id IN(SELECT\n",
    "                            DISTINCT c.concept_id \n",
    "                        FROM\n",
    "                            `cb_criteria` c \n",
    "                        JOIN\n",
    "                            (SELECT\n",
    "                                CAST(cr.id as string) AS id       \n",
    "                            FROM\n",
    "                                `cb_criteria` cr       \n",
    "                            WHERE\n",
    "                                concept_id IN (201820)       \n",
    "                                AND full_text LIKE '%_rank1]%'      ) a \n",
    "                                ON (c.path LIKE CONCAT('%.', a.id, '.%') \n",
    "                                OR c.path LIKE CONCAT('%.', a.id) \n",
    "                                OR c.path LIKE CONCAT(a.id, '.%') \n",
    "                                OR c.path = a.id) \n",
    "                        WHERE\n",
    "                            is_standard = 1 \n",
    "                            AND is_selectable = 1) \n",
    "                        AND is_standard = 1 )) criteria ) )\n",
    "        )\", sep=\"\")\n",
    "\n",
    "# Formulate a Cloud Storage destination path for the data exported from BigQuery.\n",
    "# NOTE: By default data exported multiple times on the same day will overwrite older copies.\n",
    "#       But data exported on a different days will write to a new location so that historical\n",
    "#       copies can be kept as the dataset definition is changed.\n",
    "survey_89304976_path <- file.path(\n",
    "  Sys.getenv(\"WORKSPACE_BUCKET\"),\n",
    "  \"bq_exports\",\n",
    "  Sys.getenv(\"OWNER_EMAIL\"),\n",
    "  strftime(lubridate::now(), \"%Y%m%d\"),  # Comment out this line if you want the export to always overwrite.\n",
    "  \"survey_89304976\",\n",
    "  \"survey_89304976_*.csv\")\n",
    "message(str_glue('The data will be written to {survey_89304976_path}. Use this path when reading ',\n",
    "                 'the data into your notebooks in the future.'))\n",
    "\n",
    "# Perform the query and export the dataset to Cloud Storage as CSV files.\n",
    "# NOTE: You only need to run `bq_table_save` once. After that, you can\n",
    "#       just read data from the CSVs in Cloud Storage.\n",
    "bq_table_save(\n",
    "  bq_dataset_query(Sys.getenv(\"WORKSPACE_CDR\"), dataset_89304976_survey_sql, billing = Sys.getenv(\"GOOGLE_PROJECT\")),\n",
    "  survey_89304976_path,\n",
    "  destination_format = \"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data directly from Cloud Storage into memory.\n",
    "# NOTE: Alternatively you can `gsutil -m cp {survey_89304976_path}` to copy these files\n",
    "#       to the Jupyter disk.\n",
    "read_bq_export_from_workspace_bucket <- function(export_path) {\n",
    "  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character())\n",
    "  bind_rows(\n",
    "    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),\n",
    "        function(csv) {\n",
    "          message(str_glue('Loading {csv}.'))\n",
    "          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)\n",
    "          if (is.null(col_types)) {\n",
    "            col_types <- spec(chunk)\n",
    "          }\n",
    "          chunk\n",
    "        }))\n",
    "}\n",
    "dataset_89304976_survey_df <- read_bq_export_from_workspace_bucket(survey_89304976_path)\n",
    "\n",
    "dim(dataset_89304976_survey_df)\n",
    "\n",
    "head(dataset_89304976_survey_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's clean and join our data before saving to our workspace bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Sort our survey data by *person_id***\n",
    "\n",
    "Create a new object called survey_arranged using the assignment (<-) operator\n",
    "\n",
    "Then, we will use the *arrange()* function from the **Dplyr** package within the **Tidyverse** (loaded earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "survey_arranged <- dataset_89304976_survey_df %>%\n",
    "arrange(person_id)\n",
    "\n",
    "dim(survey_arranged)\n",
    "head(survey_arranged, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Clean up the answer column**\n",
    "\n",
    "We will also create a new object called survey_clean which is modifies the previous survey_arranged data frame\n",
    "\n",
    "As you can see in the previous chunk, the answer column includes the same string of text found in the question column which we want to **remove**\n",
    "\n",
    "We will use the *str_remove()* and *str_trim()* functions from the **stringr** package within the **Tidyverse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "survey_clean <- survey_arranged %>%\n",
    "  mutate(\n",
    "    answer = str_remove(answer, fixed(question)), # remove the exact question text (literal match)\n",
    "    answer = str_remove(answer, \"^\\\\s*-\\\\s*\"),    # remove a leading hyphen and surrounding spaces left behind\n",
    "    answer = str_trim(answer)                     # trim any leftover whitespace\n",
    "  )\n",
    "\n",
    "head(survey_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Make the values in the question column the column names and the values in the answer column the values for the new columns**\n",
    "\n",
    "This will get rid of the person_id duplicates but keep both the questions and their responses\n",
    "\n",
    "We will create a new object called survey_wide which is modifies the previous survey_clean data frame\n",
    "\n",
    "We will use the *pivot_wider()* function from the **tidyr** package within the **Tidyverse** to accomplish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "survey_wide <- survey_clean %>%\n",
    "  pivot_wider(\n",
    "    names_from = question,\n",
    "    values_from = answer\n",
    "  )\n",
    "\n",
    "dim(survey_wide)\n",
    "head(survey_wide, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Join the updated survey data frame, survey_wide, to our demographic variable data frame *dataset_89304976_person_df***\n",
    "\n",
    "We will create a new object called diabetes_study_ptsd which will include variables from our survey and demographic variable data frames.\n",
    "\n",
    "We will use the *left_join()* function from the **dplyr** package to join the two data frames on the **person_id** variable.\n",
    "\n",
    "We will also use the *filter()* function from the **dplyr** package to remove NA values from our new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "\n",
    "diabetes_study_ptsd <- dataset_89304976_person_df %>%\n",
    "  left_join(survey_wide, by = c(\"person_id\" = \"person_id\")) %>%\n",
    "  filter(\n",
    "    !is.na(`Are you still seeing a doctor or health care provider for post-traumatic stress disorder (PTSD)?`),\n",
    "    !is.na(`Are you currently prescribed medications and/or receiving treatment for post-traumatic stress disorder (PTSD)?`)\n",
    "  )\n",
    "\n",
    "dim(diabetes_study_ptsd)\n",
    "head(diabetes_study_ptsd, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the provided snippets to save our cleaned data frame to our workspace bucket\n",
    "\n",
    "This section saves R data frames that are already in memory to our workspace bucket using a two-step process:\n",
    "\n",
    "1. Writes dataframe to local CSV file on persistent disk\n",
    "2. Copies file from local storage to workspace bucket using gsutil\n",
    "\n",
    "It also verifies successful upload by listing bucket contents\n",
    "\n",
    "Once our final data are saved to the workspace bucket, we no longer need our persistent disk\n",
    "\n",
    "**MAY HAVE TO RUN SET UP SNIPPET FIRST BUT ITS JUST TO LOAD TIDYVERSE WHICH WE DID ALREADY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# This snippet assumes that you run setup first\n",
    "\n",
    "# This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
    "\n",
    "# Replace df with THE NAME OF YOUR DATAFRAME\n",
    "my_dataframe <- diabetes_study_ptsd\n",
    "\n",
    "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "destination_filename <- 'diabetes_study_ptsd.csv'\n",
    "\n",
    "########################################################################\n",
    "##\n",
    "################# DON'T CHANGE FROM HERE ###############################\n",
    "##\n",
    "#######################################################################\n",
    "\n",
    "# store the dataframe in current workspace\n",
    "write_excel_csv(my_dataframe, destination_filename)\n",
    "\n",
    "# Get the bucket name\n",
    "my_bucket <- Sys.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# Copy the file from current workspace to the bucket\n",
    "system(paste0(\"gsutil cp ./\", destination_filename, \" \", my_bucket, \"/data/\"), intern=T)\n",
    "\n",
    "# Check if file is in the bucket\n",
    "system(paste0(\"gsutil ls \", my_bucket, \"/data/*.csv\"), intern=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
